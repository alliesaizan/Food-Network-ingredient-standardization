{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredient Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the exercise is to identify similar ingredients and replace them with one main ingredient. This exercise will reduce the dimensionality of an ingredients list, which can then be piped into other methods for processing. \n",
    "\n",
    "The main task is to identify the **semantic similarity** of ingredients. Semantic similarity falls under the more general concept of semantic relatedness. Semantic relatedness refers to measuring any relationship between two words (such as \"rain\" and \"flood\"). Semantic similarity more specifically measures the interchangeability of words (such as \"bank\" and \"trust company\"). I will work with semantic similarity instead of semantic relatedness for this analysis because I want to determine which ingredients are essentially interchangeable, as opposed to ingredients that are parts of, or related to, other ingredients.\n",
    "\n",
    "Within text analysis exist, two main methods to determine semantic similarity. The first method is **corpus-based semantic similarity**. Corpus-based (or \"information content\") methods measure the differences in information content between two words or phrases as a function of their probability of occurrence in a corpus. The second method, **edge counting semantic similarity**, defines text similarity as a function of the length of a path linking two words or phrases together and on the position of the word or phrases in a taxonomy. Edge counting methods typically use hierarchical word networks, such as WordNet, to determine path similarities.\n",
    "\n",
    "This exercise is challenging because of many factors. Ingredients are generally not single words, but are word phrases. Additionally, a great amount of variation exists within a particular ingredient type. For example, [give example]. \n",
    "\n",
    "In this analysis, I will attempt both methods of semantic similarity. First, I will derive cosine similarity of every ingredient with every other ingredient in the ingredient list. For each ingredient, I extract the top ten similarities. Second, I will attempt to cull the list of similarities obtained by the cosine similarity exercise using a path similarities computational algorithm. I believe computing path similarities for each ingredient paring in the list would be too computationally intensive.\n",
    "\n",
    "Both semantic similarity methods are essentially grouping ingredients togther. Measures of semantic similarity are not straightforward to validate. As an alternative method of developing and validating groupings, my third step will be to perform a k-means clustering. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import re\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON ingredient data\n",
    "recipes = pd.read_json(\"recipes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation set\n",
    "validationSet = pickle.load(open(\"validationSet.py\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text and prepping it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull ingredients into a list\n",
    "ingredients = list(chain.from_iterable(recipes.ingredients.tolist()))\n",
    "ingredientsLower = [i.lower() for i in ingredients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6714"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract unique ingredients\n",
    "ingredientsLower = list(set(ingredients))\n",
    "len(ingredientsLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6559"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the validation (test) ingredients from the main dataset\n",
    "ingredientsLower_train = [i for i in ingredientsLower if i not in validationSet] \n",
    "len(ingredientsLower_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify words in the rightmost stage of the phrase (the \"anchor\" words)\n",
    "last_tokens = [i.lower().split()[-1] for i in ingredientsLower_train]\n",
    "last_tokens = set(last_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words and take the unique entries\n",
    "ps = PorterStemmer()\n",
    "last_tokens = set([ps.stem(w) for w in last_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization (Word Analyzer) and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in any text analysis is to vectorize words. I vectorize the ingredients using the TF-IDF algorithm. TF-IDF is a commonly used word vectorizer. In the TF-IDF vectorizer call, the \"text analyzer\" we will use is \"word;\" essentially, I am using the word as the elements that populate the vectors. Alternative analyzers are at the character- and n-gram-level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a TF-IDF matrix of vectors to which we can apply the cosine similarity algorithm\n",
    "vectorizer = TfidfVectorizer(min_df =1, analyzer = \"word\")\n",
    "tfidf = vectorizer.fit_transform(ingredientsLower_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity, the linear kernel dot product is functionally the same as the cosine similarity, because the TF-IDF vectors are normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cosine similarity of the first ingredient (\"document\") with every other ingredient    \n",
    "cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0, 5412, 2995, 5509])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the top five matches?\n",
    "indicies = cosine_similarities.argsort()[:-5:-1]\n",
    "indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll loop over all entries in the tfidf matrix and pull out the top ten similarities. I am finding similarities this way because computing the TF-IDF matrix of every document with every other document gave me nonsense indicies that did not correspond to the ingredients dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "\n",
    "for i in range(0, len(ingredientsLower_train) - 1 ):\n",
    "    cosine_similarities = linear_kernel(tfidf[i], tfidf).flatten()\n",
    "    indicies = cosine_similarities.argsort()[:-10:-1]\n",
    "    similarities.append({ingredientsLower_train[i] : [ingredientsLower_train[i] for i in indicies]})\n",
    "\n",
    "del i, vectorizer, tfidf, cosine_similarities, indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to validate the effectiveness of this method. I'll create a data frame where each row is a match between ingredients. I need a way to determine whether these methods are valid. From inspection, it seems like each ingredient phrase has an \"anchor\" ingredient. I will judge matches as good, then, if they share the same anchor ingredient and if the TF-IDF algorithm placed them in the top ten. This may not be the best measure of positive matches, but it's the easiest to derive a confusion matrix from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>[pork tongue, tongue, beef tongue, veal tongue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chicken stock cubes</td>\n",
       "      <td>[chicken stock cubes, Knorr Chicken Stock Cube...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boneless skinless chicken</td>\n",
       "      <td>[boneless skinless chicken, boneless, skinless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poolish</td>\n",
       "      <td>[poolish, nut oil, cayenne pepper sauce, sprin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>banh hoi</td>\n",
       "      <td>[banh hoi, banh pho rice noodles, nut oil, bla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         key  \\\n",
       "0                pork tongue   \n",
       "1        chicken stock cubes   \n",
       "2  boneless skinless chicken   \n",
       "3                    poolish   \n",
       "4                   banh hoi   \n",
       "\n",
       "                                               match  \n",
       "0  [pork tongue, tongue, beef tongue, veal tongue...  \n",
       "1  [chicken stock cubes, Knorr Chicken Stock Cube...  \n",
       "2  [boneless skinless chicken, boneless, skinless...  \n",
       "3  [poolish, nut oil, cayenne pepper sauce, sprin...  \n",
       "4  [banh hoi, banh pho rice noodles, nut oil, bla...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull the ingredient list into a data frame of pairwise combinations\n",
    "ingredientMatches = pd.DataFrame(columns = ['key', 'match'])\n",
    "for match in similarities:\n",
    "    ingredientMatches = ingredientMatches.append({'key': \"\".join(match.keys()), 'match': [i for i in match.values()][0]}, ignore_index = True)\n",
    "ingredientMatches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>pork tongue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>tongue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>beef tongue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>veal tongue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>pork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>ox tongue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>ground pork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>salt pork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>pork stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chicken stock cubes</td>\n",
       "      <td>chicken stock cubes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chicken stock cubes</td>\n",
       "      <td>Knorr Chicken Stock Cubes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    key                      match\n",
       "0           pork tongue                pork tongue\n",
       "1           pork tongue                     tongue\n",
       "2           pork tongue                beef tongue\n",
       "3           pork tongue                veal tongue\n",
       "4           pork tongue                       pork\n",
       "5           pork tongue                  ox tongue\n",
       "6           pork tongue                ground pork\n",
       "7           pork tongue                  salt pork\n",
       "8           pork tongue                 pork stock\n",
       "9   chicken stock cubes        chicken stock cubes\n",
       "10  chicken stock cubes  Knorr Chicken Stock Cubes"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unnest the list to obtain pairwise matches and drop duplicates\n",
    "ingredientMatches = ingredientMatches.set_index(\"key\").apply(lambda x: x.apply(pd.Series).stack()).reset_index(level=1, drop=True).reset_index()\n",
    "ingredientMatches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59022, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredientMatches = ingredientMatches.drop_duplicates()\n",
    "ingredientMatches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>match</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>false_pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>pork tongue</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>tongue</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>beef tongue</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>veal tongue</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pork tongue</td>\n",
       "      <td>pork</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key        match  true_pos  false_pos  tag\n",
       "0  pork tongue  pork tongue      True      False    1\n",
       "1  pork tongue       tongue      True      False    1\n",
       "2  pork tongue  beef tongue      True      False    1\n",
       "3  pork tongue  veal tongue      True      False    1\n",
       "4  pork tongue         pork     False       True    1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new column which identifies whether a match is valid\n",
    "ingredientMatches['true_pos'] = ingredientMatches['key'].str.split().apply(lambda x: x[-1]) == ingredientMatches['match'].str.split().apply(lambda x: x[-1])\n",
    "ingredientMatches['false_pos'] = ingredientMatches['key'].str.split().apply(lambda x: x[-1]) != ingredientMatches['match'].str.split().apply(lambda x: x[-1])\n",
    "ingredientMatches['tag'] = 1\n",
    "ingredientMatches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6fb77097f511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mingredientsLower_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mingredientMatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingredientMatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"match\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true_pos\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"false_pos\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tag\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6183\u001b[0m             other = DataFrame(other.values.reshape((1, len(other))),\n\u001b[1;32m   6184\u001b[0m                               \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6185\u001b[0;31m                               columns=combined_columns)\n\u001b[0m\u001b[1;32m   6186\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 379\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4857\u001b[0m                                      placement=slice(0, len(axes[0])))]\n\u001b[1;32m   4858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4859\u001b[0;31m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4860\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4861\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3282\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3489\u001b[0;31m         \u001b[0mmgr_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3490\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3491\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3307\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3309\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3311\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Append the rest of the ingredient-matches to the ingredient matches list\n",
    "keylist = ingredientMatches.drop_duplicates(subset = \"key\").key.tolist()\n",
    "temp = list(ingredientMatches[['key', 'match']].itertuples(index = False, name = None))\n",
    "\n",
    "for item in keylist:\n",
    "    for i in ingredientsLower_train:\n",
    "        if (item, i) not in temp:\n",
    "            ingredientMatches = ingredientMatches.append({\"key\": item, \"match\": i, \"true_pos\": False, \"false_pos\": False, \"tag\": 0}, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bde26b57e4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate true and false negatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mingredientMatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'true_neg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingredientMatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mingredientMatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'match'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mingredientMatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mingredientMatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'false_neg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingredientMatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mingredientMatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'match'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mingredientMatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1571\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m   1572\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Calculate true and false negatives\n",
    "ingredientMatches['true_neg'] = ingredientMatches['key'].str.split().apply(lambda x: x[-1]) != ingredientMatches['match'].str.split().apply(lambda x: x[-1]) and ingredientMatches['tag'] ==0\n",
    "ingredientMatches['false_neg'] = ingredientMatches['key'].str.split().apply(lambda x: x[-1]) == ingredientMatches['match'].str.split().apply(lambda x: x[-1]) and ingredientMatches['tag'] ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(ingredientMatches, \"ingredientMatches.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_rate = ingredientMatches['true_pos'].sum() /  ingredientMatches['true_pos'].sum() + ingredientMatches['false_neg'].sum()\n",
    "false_positive_rate = ingredientMatches['false_pos'].sum() /  ingredientMatches['false_pos'].sum() + ingredientMatches['true_neg'].sum()\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization (\"Anchor\" Analyzer) and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify ingredients which contain the anchor word\n",
    "position = []\n",
    "for i in last_tokens:\n",
    "    if i in vectorizer.vocabulary_.keys():\n",
    "        position.append(vectorizer.vocabulary_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Place greater weights on entries in the TF-IDF matrix that contain the anchor word. The weight is notional\n",
    "tfidf[:, position] *= 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain word similarities\n",
    "similarities = []\n",
    "\n",
    "for i in range(0, len(ingredientsLower_train) - 1 ):\n",
    "    cosine_similarities = linear_kernel(tfidf[i], tfidf).flatten()\n",
    "    indicies = cosine_similarities.argsort()[:-5:-1]\n",
    "    similarities.append({ingredientsLower_train[i] : [ingredientsLower_train[i] for i in indicies]})\n",
    "\n",
    "del i, vectorizer, tfidf, cosine_similarities, indicies\n",
    "\n",
    "similarities[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I still did not realize when implementing this was that this is essentially just scaling the weights on all the entries up, since all the ingedients have an \"anchor\" ingredient. One to-do item for me is to find a smarter weighting method that differentially weights anchor ingredients higher if they're already assigned as matches to the main ingredient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where I'm left is with a bunch of matches to a particular ingredient, some of which are relevant and some of which are not. What are ways we can weed out the irrelevant ingredients? One thought is to use Wordnet synsets. NLTK's Wordnet interface has functions to compute path similarities, as well as determine word hyponyms and hypernyms. The first method I will try is to weed out matched ingredients with a path similarity from the anchor word that's lower than some arbitrary threshold. Another method for determining \"families\" of words is to compare hypernyms of words within a phrase and only keep words under the same family of words. Instructions on using the NLTK implementation of Wordnet can be found here: http://www.nltk.org/howto/wordnet.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using synsets as a validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which determines the synsets of the rightmost word\n",
    "def find_synset(split):\n",
    "    for i in range(1, len(split)+1):        \n",
    "        try:\n",
    "            synset = wn.synset(split[-i] +'.n.01')\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "    return [synset, anchor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to further weed out irrelevant entries identified by TF-IDF\n",
    "newSimilarities = {}\n",
    "def cull_similarities(textbox):\n",
    "    global newSimilarities\n",
    "    for i in range(0, len(textbox)):\n",
    "        keep = []\n",
    "        text = \"\".join(textbox[i].keys()).split()\n",
    "        key = \"\".join(textbox[i].keys())\n",
    "\n",
    "        # Determine rightmost viable ingredient and its synset\n",
    "        test = ps.stem(text[-1])\n",
    "        anchor = wn.synset(test +'.n.01')\n",
    "        print(anchor)\n",
    "    \n",
    "        # Compare matches\n",
    "        matches = [i for i in textbox[i].values()][0]\n",
    "    \n",
    "        for i in matches:\n",
    "            paths = []\n",
    "            if i.split()[-1] == anchor.lemma_names()[0]:\n",
    "                keep.append(i)\n",
    "            else:\n",
    "                try:\n",
    "                     for j in i.split():\n",
    "                         paths.append(anchor.path_similarity(wn.synset(j + '.n.01')))\n",
    "                except:\n",
    "                    paths.append(0)\n",
    "            #print(i + \": \" + str(np.mean(paths)))\n",
    "            if np.mean(paths) >= 0.08:\n",
    "                keep.append(i)\n",
    "        # Append matches to a new dictionary\n",
    "        newSimilarities[key] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cull_similarities(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text clustering uses a text vector to identify groupings with similar patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: http://brandonrose.org/clustering#K-means-clustering\n",
    "k = 100 # Generate 100 clusters; this number is arbitrary and reduces the work on me to check the clusters\n",
    "km = KMeans(n_clusters = k)\n",
    "km.fit(tfidf)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify ingredients that characterize each cluster\n",
    "temp = pd.DataFrame({\"ingredients\": ingredientsLower_train})\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d ingredients:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :5]: \n",
    "        print(' %s' % temp.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
