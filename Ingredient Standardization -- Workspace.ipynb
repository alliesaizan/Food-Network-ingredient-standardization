{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredient Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import re\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load JSON ingredient data\n",
    "recipes = pd.read_json(\"Documents/Food-Network/recipes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load validation set\n",
    "validationSet = pickle.load(open(\"Documents/Food-Network/validationSet.py\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the text and prepping it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull ingredients into a list\n",
    "ingredients = list(chain.from_iterable(recipes.ingredients.tolist()))\n",
    "ingredientsLower = [i.lower() for i in ingredients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract unique ingredients\n",
    "ingredientsLower = list(set(ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify words in the rightmost stage of the phrase\n",
    "last_tokens = [i.lower().split()[-1] for i in ingredientsLower]\n",
    "last_tokens = set(last_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem words and take the unique entries\n",
    "ps = PorterStemmer()\n",
    "last_tokens = set([ps.stem(w) for w in last_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: TF-IDF Vectorization (Word Analyzer) and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the TF-IDF vectorizer call, the \"text analyzer\" we will use is \"word.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain a TF-IDF matrix of vectors\n",
    "    # to which we can apply the cosine similarity algorithm\n",
    "vectorizer = TfidfVectorizer(min_df =1, analyzer = \"word\")\n",
    "tfidf = vectorizer.fit_transform(ingredientsLower)\n",
    "\n",
    "print(tfidf[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity, the linear kernel dot product is functionally the same as the cosine similarity, because the TF-IDF vectors are normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the cosine similarity of the first ingredient (\"document\") with every other ingredient    \n",
    "cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What are the top five matches?\n",
    "indicies = cosine_similarities.argsort()[:-5:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll loop over all entries in the tfidf matrix and pull out the top five similarities. I am finding similarities this way because computing the TF-IDF matrix of every document with every other document gave me nonsense indicies that did not correspond to the ingredients dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarities = []\n",
    "\n",
    "for i in range(0, len(ingredientsLower) - 1 ):\n",
    "    cosine_similarities = linear_kernel(tfidf[i], tfidf).flatten()\n",
    "    indicies = cosine_similarities.argsort()[:-5:-1]\n",
    "    similarities.append({ingredientsLower[i] : [ingredientsLower[i] for i in indicies]})\n",
    "\n",
    "del i, vectorizer, tfidf, cosine_similarities, indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check the accuracy of the matches\n",
    "similarities[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this method kinda works. But looking at the third entry in the similarities list, \"pineapple rings\" is matching with \"onion rings\" and \"hot pepper rings\", which is clearly inaccurate. Let's try this method with a different analyzer-- the n-gram analyzer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: TF-IDF Vectorization (N-gram Analyzer) and Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing method 1 using an n-gram analyzer\n",
    "    # Adapted from https://bergvca.github.io/2017/10/14/super-fast-string-matching.html\n",
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df =1, analyzer = ngrams)\n",
    "tfidf = vectorizer.fit_transform(ingredientsLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify the similarity of every ingredient with every other ingredient and record the top five matches\n",
    "similarities = []\n",
    "\n",
    "for i in range(0, len(ingredientsLower) - 1 ):\n",
    "    cosine_similarities = linear_kernel(tfidf[i], tfidf).flatten()\n",
    "    indicies = cosine_similarities.argsort()[:-5:-1]\n",
    "    similarities.append({ingredientsLower[i] : [ingredientsLower[i] for i in indicies]})\n",
    "\n",
    "similarities[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looking at the third entry in similarities again, we are getting more similar results. But how swappable is \"pineapple juice\" with \"pineapple salsa\"? The TF-IDF method is still not quite working. One solution is to place  greater weights on words that appear at the end of a phrase. We will use the \"word\" analyzer to implement this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: TF-IDF Vectorization (\"Anchor\" Analyzer) and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain a TF-IDF matrix of vectors\n",
    "    # to which we can apply the cosine similarity algorithm\n",
    "vectorizer = TfidfVectorizer(min_df =1, analyzer = \"word\")\n",
    "tfidf = vectorizer.fit_transform(ingredientsLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify ingredients which contain the anchor word\n",
    "position = []\n",
    "for i in last_tokens:\n",
    "    if i in vectorizer.vocabulary_.keys():\n",
    "        position.append(vectorizer.vocabulary_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Place greater weights on entries in the TF-IDF matrix that contain the anchor word. The weight is notional\n",
    "tfidf[:, position] *= 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain word similarities\n",
    "similarities = []\n",
    "\n",
    "for i in range(0, len(ingredientsLower) - 1 ):\n",
    "    cosine_similarities = linear_kernel(tfidf[i], tfidf).flatten()\n",
    "    indicies = cosine_similarities.argsort()[:-5:-1]\n",
    "    similarities.append({ingredientsLower[i] : [ingredientsLower[i] for i in indicies]})\n",
    "\n",
    "del i, vectorizer, tfidf, cosine_similarities, indicies\n",
    "\n",
    "similarities[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I still did not realize when implementing this was that this is essentially just scaling the weights on all the entries up, since all the ingedients have an \"anchor\" ingredient. One to-do item for me is to find a smarter weighting method that differentially weights anchor ingredients higher if they're already assigned as matches to the main ingredient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where I'm left is with a bunch of matches to a particular ingredient, some of which are relevant and some of which are not. What are ways we can weed out the irrelevant ingredients? One thought is to use Wordnet synsets. NLTK's Wordnet interface has functions to compute path similarities, as well as determine word hyponyms and hypernyms. The first method I will try is to weed out matched ingredients with a path similarity from the anchor word that's lower than some arbitrary threshold. Another method for determining \"families\" of words is to compare hypernyms of words within a phrase and only keep words under the same family of words. Instructions on using the NLTK implementation of Wordnet can be found here: http://www.nltk.org/howto/wordnet.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to further weed out irrelevant entries identified by TF-IDF\n",
    "\n",
    "def cutter(textbox):\n",
    "    for i in range(0, len(textbox)):\n",
    "        cut = []\n",
    "        anchor = \"\".join(textbox[i].keys()).split()[1]\n",
    "        matches = [i for i in textbox[i].values()][0]\n",
    "        for i in matches:\n",
    "            if i.split()[1] == anchor:\n",
    "                cut.append(i)\n",
    "            else:\n",
    "                i = i.split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
